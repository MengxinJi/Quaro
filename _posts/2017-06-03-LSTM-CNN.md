---
title:  "LSTM CNN‚Äù
date:   2017-06-03 22:37:00
categories: LSTM
---

```python
import sys
import os  
import tensorflow as tf
import pickle as plk 
import numpy as np  
import pandas as pd
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Lambda, merge, Activation, Input, Merge
from keras import backend as K
from keras.optimizers import RMSprop, SGD
import keras.layers as lyr
from keras.models import Model
from keras.optimizers import Adam
from keras.layers import *
from keras.callbacks import *

######################################################################
######################################################################

with open('Xbasic_xgb.p', 'rb') as f:
    Xbasic= plk.load(f)
    
    
Xbasic_train = Xbasic['Xbasic_train']
Xbasic_val = Xbasic['Xbasic_val']
Xbasic_test = Xbasic['Xbasic_test']
y_train = Xbasic['y_train']
y_val = Xbasic['y_val']
test_xgb = Xbasic['test_xgb']
val_xgb = Xbasic['val_xgb']
test_id = Xbasic['test_id']

######################################################################
######################################################################

myindex = 6

epochs = 500
patience = 5


# Transfer before merge
lr = 1e-3

Google = True
emb_trainable = False

trans_merge = False
input_extrange = not trans_merge

use_conv = False
use_pre = True

fc_res = True
conv_res = True

drop_rate = 0.2
lstm_dim = 256
conv_dim = 256
after_dim = 256
lstm_act = 'tanh'

re_weight_train = False
re_weight_val = False

    
if re_weight_train:
    class_weight = {0: 1.309028344, 1: 0.472001959}
else:
    class_weight = None




if Google:
    print("Use embedding by Google, trainable:" + str(emb_trainable))

    with open('save_objs_google.p', 'rb') as f:
        lst = plk.load(f)
else:
    print("Use embedding by Glove, trainable:" + str(emb_trainable))
    with open('save_objs.p', 'rb') as f:
        lst = plk.load(f)


X1_train, X1_val, X2_train, X2_val,y_train, y_val, emb_matrix = \
                                                                lst['X1_train'],lst['X1_val'], lst['X2_train'], lst['X2_val'], lst['y_train'],\
                                                                lst['y_val'], lst['embedding_matrix']



X1_test,  X2_test = \
                    lst['test_q1'],lst['test_q2']

from numpy import vstack

n_val = len(y_val)
n_test = X1_test.shape[0]

print(n_val)
print(n_test)
print(Xbasic_train)
print(Xbasic_train.shape)


from sklearn.preprocessing import normalize



Xbasic_train = pd.DataFrame(Xbasic_train)
Xbasic_train = Xbasic_train.replace([np.inf, -np.inf], np.nan)
Xbasic_train = Xbasic_train.dropna(axis = 1)

Xbasic_val = pd.DataFrame(Xbasic_val)
Xbasic_val = Xbasic_val.replace([np.inf, -np.inf], np.nan)
Xbasic_val = Xbasic_val.dropna(axis = 1)

Xbasic_test = pd.DataFrame(Xbasic_test)
Xbasic_test = Xbasic_test.replace([np.inf, -np.inf], np.nan)
Xbasic_test = Xbasic_test.dropna(axis = 1)


# Xbasic_train = normalize(Xbasic_train.astype(float), axis=1, norm='l2')
# Xbasic_val = normalize(Xbasic_val.astype(float) , axis=1, norm='l2')
# Xbasic_test = normalize(Xbasic_test.astype(float), axis=1, norm='l2')

if input_extrange:
    X1_train, X2_train, y_train = vstack((X1_train, X2_train)), vstack((X2_train, X1_train)), np.concatenate((y_train, y_train), axis=0)
    X1_val, X2_val, y_val = vstack((X1_val, X2_val)), vstack((X2_val, X1_val)), np.concatenate((y_val, y_val), axis=0)
    Xbasic_train, Xbasic_val, Xbasic_test = vstack((Xbasic_train, Xbasic_train)),  vstack((Xbasic_val, Xbasic_val)), vstack((Xbasic_test, Xbasic_test))
    X1_test, X2_test = vstack((X1_test, X2_test)), vstack((X2_test, X1_test))

test_id = lst['test_id']

MAX_NB_WORDS = emb_matrix.shape[0]

weight_val = np.ones(len(y_val))

if re_weight_val:
    weight_val *= 0.472001959
    weight_val[y_val==0] = 1.309028344


print(X1_train.shape)
print(X1_test.shape)
print(y_val.shape)
##########################################################################################
##########################################################################################

# Define layers
print(X1_train.shape)

words_embedding_layer = lyr.Embedding(MAX_NB_WORDS, 300, 
                                      weights=[emb_matrix],
                                      trainable = emb_trainable)

# 
seq_embedding_layer = lyr.wrappers.Bidirectional(lyr.LSTM(lstm_dim,
                                                          dropout= drop_rate,
                                                          activation= lstm_act,
                                                          recurrent_dropout= drop_rate,
                                                          return_sequences= use_conv))


seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))


conv_layer1 = lyr.Conv1D(conv_dim, 4, activation='linear', padding= 'same')
max_pool1 = lyr.MaxPooling1D(pool_size = 6)
relu_layer1 = lyr.Activation('relu')

conv_layer2 = lyr.Conv1D(conv_dim, 4, activation='linear', padding= 'same')
max_pool2 = lyr.MaxPooling1D(pool_size = 6)
relu_layer2 = lyr.Activation('relu')
flate_layer = lyr.Flatten()




def conv_embedding(tensor):
    output1 = conv_layer1(tensor)
    # if conv_res:
    #     output1 = merge([tensor, output1], mode='sum')
    output1 = relu_layer1(output1)
    output1 = max_pool1(output1)

    output2 = conv_layer2(output1)
    if conv_res:
        output2 = merge([output1, output2], mode='sum')
    output2 = relu_layer1(output2)
    output2 = max_pool1(output2)

    output = flate_layer(output2)
    return(output)


pre1 = lyr.Dense(conv_dim, activation = 'relu')
pre2 = Dropout(drop_rate)
pre3 = lyr.normalization.BatchNormalization()

def pre_layer(tensor):
    output = pre1(tensor)

    # if fc_res:
    #     output = merge([tensor, output], mode='sum')
    output = pre3(pre2(output))
    return(output)






after11 = lyr.Dense(conv_dim, activation = 'relu')
after12= Dropout(drop_rate)
after13 = lyr.normalization.BatchNormalization()

def after_layer1(tensor):
    output = after11(tensor)
    # the size is changed in this layer
    # if fc_res:
    #     output = merge([tensor, output], mode='sum')
    output = after13(after12(output))
    return(output)


after21 = lyr.Dense(conv_dim, activation = 'relu')
after22= Dropout(drop_rate)
after23 = lyr.normalization.BatchNormalization()

def after_layer2(tensor):
    output = after21(tensor)
    if fc_res:
        output = merge([tensor, output], mode='sum')
    output = after23(after22(output))
    return(output)


# Build graph

input1_tensor = lyr.Input(X1_train.shape[1:], dtype='int32')
input2_tensor = lyr.Input(X2_train.shape[1:], dtype='int32')



basic_pre1 = lyr.Dense(48, activation = 'relu')
basic_pre2 = Dropout(drop_rate)
basic_pre3 = lyr.normalization.BatchNormalization()
def basic_pre_layer(tensor):
    output = basic_pre1(tensor)

    output = basic_pre3(basic_pre2(output))
    return(output)

input3_tensor = lyr.Input(Xbasic_train.shape[1:])
basic_feats = basic_pre_layer(input3_tensor)

if use_conv:
    seq1 = conv_embedding(seq_embedding(input1_tensor))
    seq2= conv_embedding(seq_embedding(input2_tensor))
else:
    seq1 = seq_embedding(input1_tensor)
    seq2= seq_embedding(input2_tensor)


if use_pre:
    seq1 = pre_layer(seq1)
    seq2 = pre_layer(seq2)

if trans_merge:
    merge1 = lyr.merge([seq1, seq2], mode = 'mul')
    merge2 = lyr.merge([seq1, seq2], mode=lambda x: abs(x[0] - x[1]), output_shape=lambda x: x[0])

    merged = lyr.concatenate([merge1, merge2, basic_feats])
else:

    merged = lyr.concatenate([seq1, seq2, basic_feats])


merged = after_layer1(merged)
merged =  after_layer2(merged)

ouput_layer = lyr.Dense(1, activation='sigmoid')(merged)


########################################
## train the model
########################################
model = Model([input1_tensor, input2_tensor, input3_tensor], ouput_layer)
model.summary()


optimizer =  Adam(lr = lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-4)
model.compile(loss='binary_crossentropy',
        optimizer= optimizer,
        metrics=['acc'])



save_best_weights = 'LSTM_CNN_basic' + str(myindex) + '.h5'

early_stopping =EarlyStopping(monitor='val_acc',
                              patience= patience)
model_checkpoint = ModelCheckpoint(save_best_weights,
                                   save_best_only=True,
                                   save_weights_only=True)

hist = model.fit([X1_train, X2_train, Xbasic_train], y_train, \
        validation_data=([X1_val, X2_val, Xbasic_val], y_val, weight_val), \
        epochs=200, batch_size=2048, shuffle=True, \
        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])

model.load_weights(save_best_weights)
bst_val_score = min(hist.history['val_loss'])


print('Making prediction for the validation set')
pred_val = np.squeeze(model.predict([X1_val, X2_val, Xbasic_val], batch_size=4000))
print('Making prediction for the testing set')
pred_test = np.squeeze( model.predict([X1_test, X2_test, Xbasic_test], batch_size=4000))




if input_extrange:
    pred_val = (pred_val[:n_val] +  pred_val[n_val:]) / 2
    pred_test = (pred_test[:n_test] +  pred_test[n_test:]) / 2



predictions = {'pred_val' : pred_val,
            'pred_test' : pred_test,
            'y_val' : y_val}

import pickle
pickle.dump(predictions, open( 'LSTM_CNN_basic'+ str(myindex) + ".p", "wb" ), protocol=2)
```
