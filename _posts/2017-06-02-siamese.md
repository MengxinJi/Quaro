---
title:  Siamese
date:   2017-06-02 22:37:00
categories: Deep Learning, Siamese
---


```python
import sys
import os  
import tensorflow as tf
import pickle as plk 
import numpy as np  
import pandas as pd
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Lambda, merge, Activation, Input, Merge
from keras import backend as K
from keras.optimizers import RMSprop, SGD
import keras.layers as lyr
from keras.models import Model
from keras.layers import *
from keras.callbacks import *


re_weight = True
if re_weight:
    class_weight = {0: 1.309028344, 1: 0.472001959}
else:
    class_weight = None


with open('save_objs.p', 'rb') as f:
    lst = plk.load(f)

X1_train, X1_val, X2_train, X2_val,y_train, y_val, emb_matrix = \
                                                                lst['X1_train'],lst['X1_val'], lst['X2_train'], lst['X2_val'], lst['y_train'],\
                                                                lst['y_val'], lst['embedding_matrix']



X1_test,  X2_test = \
                    lst['test_q1'],lst['test_q2']


test_id = \
          lst['test_id']

####################################################################################
def cosine_sim(vests):
    x, y = vests
    x = K.l2_normalize(x, axis=-1)
    y = K.l2_normalize(y, axis=-1)
    return K.mean(x * y, axis=-1, keepdims=True)

def cos_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0],1)


def contrastive_loss(y_true, y_pred):
    '''Contrastive loss from Hadsell-et-al.'06
    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
    '''
    margin = 1
    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))


####################################################################################
input1_tensor = lyr.Input(X1_train.shape[1:])
input2_tensor = lyr.Input(X2_train.shape[1:])

words_embedding_layer = lyr.Embedding(emb_matrix.shape[0], 300, 
                                      weights=[emb_matrix],
                                      trainable = True)
# 
seq_embedding_layer = lyr.wrappers.Bidirectional(lyr.LSTM(128, activation='tanh',
                                                          dropout= 0.15, 
                                                          recurrent_dropout= 0.15,
                                                          return_sequences=True))

seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))

relu_layer = lyr.Activation('relu')

conv_layer1 = lyr.Conv1D(128, 4, activation='relu', padding= 'same')
max_pool1 = lyr.MaxPooling1D(pool_size = 6)
conv_layer2 = lyr.Conv1D(128, 4, activation='relu', padding= 'same')
max_pool2 = lyr.MaxPooling1D(pool_size = 6)
flate_layer = lyr.Flatten()

conv_embedding = lambda tensor: flate_layer(
    max_pool2(
        conv_layer2(
            max_pool1(
                conv_layer1(
                    tensor
                )
            )
        )
    )
)



seq1 = conv_embedding(seq_embedding(input1_tensor))
seq2= conv_embedding(seq_embedding(input2_tensor))

dense_layer_pre1 = lyr.Dense(16, activation = 'linear')

merge_1_pre = dense_layer_pre1(seq1)
merge_1_pre2 = lyr.normalization.BatchNormalization()(merge_1_pre)
merge_1_pre3 = lyr.Activation('relu')(merge_1_pre2) 

merge_2_pre = dense_layer_pre1(seq2)
merge_2_pre2 = lyr.normalization.BatchNormalization()(merge_2_pre)
merge_2_pre3 = lyr.Activation('relu')(merge_2_pre2) 

#merge_1 = lyr.merge([seq1, seq2], mode = 'mul')
#merge_2 = lyr.merge([seq1, seq2], 
#                    mode=lambda x: abs(x[0] - x[1]), output_shape=lambda x: x[0])

distance = Lambda(cosine_sim, output_shape=cos_dist_output_shape)([merge_1_pre3 , merge_2_pre3])



####################################################################################

model = Model([input1_tensor, input2_tensor], output = distance)
model.summary()

from keras.optimizers import Adam

optimizer =  Adam(lr = 1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-4)
model.compile(loss= contrastive_loss, optimizer= 'adam', 
               metrics=['accuracy'])

save_best_weights = 'siamse_weights.h5'

callbacks = [ModelCheckpoint(save_best_weights, monitor='val_loss', save_best_only=True),
             EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')]


model.fit([X1_train, X2_train], y_train, 
          validation_data=([X1_val, X2_val], y_val), 
          batch_size=128, 
          epochs =100, 
          verbose=1, 
          class_weight = class_weight,
          shuffle=True,
          callbacks=callbacks)
```
